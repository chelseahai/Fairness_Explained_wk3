
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Extended</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Fairness Explained</h1>
    </header>
    <nav>
        <a href="index.html">Home</a>
        <a href="ontology.html">Ontology</a>
        <a href="context.html">Context</a>
        <a href="visual.html">Visual</a>
        <a href="reflection.html">Reflection</a>
        <a href="datasheet.html">Datasheet</a>
        <a href="diagrams.html">Diagrams</a>
        <a href="extended.html">Extended</a>
        <a href="methodology.html">Methodology</a>
        <a href="rhetorical.html">Rhetorical</a>
        <a href="practice.html">Practice</a>
        <a href="assessment.html">Assessment</a>
    </nav>
    <main style="background: linear-gradient(135deg, #FFF8E1, #FFECB3); position: relative;">

<svg class="dot-bg" width="100%" height="100%" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid slice" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <pattern id="dots" x="0" y="0" width="10" height="10" patternUnits="userSpaceOnUse">
      <circle cx="1" cy="1" r="1" fill="rgba(0,0,0,0.03)" />
    </pattern>
  </defs>
  <rect width="100%" height="100%" fill="url(#dots)" />
</svg>

<div style="position: relative; z-index: 1;">
        
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Extended Analysis | Fairness, Explained</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Extended Analysis</h1>
    <nav><a href="index.html">Back to Home</a></nav>
  </header>
  <main style="background: linear-gradient(135deg, #FFF8E1, #FFECB3); position: relative;">

<svg class="dot-bg" width="100%" height="100%" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid slice" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <pattern id="dots" x="0" y="0" width="10" height="10" patternUnits="userSpaceOnUse">
      <circle cx="1" cy="1" r="1" fill="rgba(0,0,0,0.03)" />
    </pattern>
  </defs>
  <rect width="100%" height="100%" fill="url(#dots)" />
</svg>

<div style="position: relative; z-index: 1;">
    <section>
      <h2>Methodology Underlying the Practice</h2>
      <p>The project employs a lightweight but sophisticated front-end software approach, using JavaScript and likely D3.js to render live visual feedback. Interactivity is central to the tool’s design: every change in the interface updates the visual logic in real time, reinforcing the project’s argument that fairness is not fixed but variable.</p>
      <p>The project deliberately avoids real-world data, opting instead for a simulated dataset. This abstraction helps clarify the underlying logic of fairness definitions but also removes historical and sociopolitical context from the discussion. The trade-off allows for conceptual clarity but sacrifices representational realism.</p>
      <p>The theoretical framework draws from machine learning fairness literature, especially research on the mathematical incompatibility of fairness definitions (such as demographic parity, equalized odds, and predictive parity). Rather than offering a definitive position, the project emphasizes interpretability, allowing users to navigate and decide what kind of fairness is appropriate in different contexts.</p>
    </section>

    <section>
      <h2>Rhetorical Analysis</h2>
      <p>The central message of “Measuring Fairness” is that fairness in machine learning is not a single objective concept but a collection of competing, often incompatible definitions. The project challenges the notion that fairness can be "solved" algorithmically and instead positions fairness as a set of ethical trade-offs that must be acknowledged and negotiated.</p>
      <p>The explorable argues that users should interact with fairness definitions, rather than merely read about them. Through interactive sliders and dynamic visualizations, it demonstrates how optimizing for one fairness metric may reduce performance on another. This rhetorical approach encourages users to engage critically and to understand fairness as a political, technical, and social decision.</p>
      <p>The project is critical in its stance toward algorithmic neutrality. By showing that metrics conflict and cannot be simultaneously satisfied, it makes visible the ethical complexity that is often hidden within machine learning systems. Rather than offering solutions, it offers clarity on the problem space, encouraging self-reflection over certainty.</p>
    </section>

    <section>
      <h2>Fit Within the Author’s Wider Practice</h2>
      <p>"Measuring Fairness" is consistent with Google PAIR’s broader mission to promote human-centered AI. The PAIR initiative has produced several related projects, including <em>Facets</em> (a visual data exploration tool), <em>Teachable Machine</em> (a no-code model training tool), and the <em>What-If Tool</em> (for exploring ML model performance through visualizations). These projects share a commitment to transparency, interactivity, and pedagogy.</p>
      <p>Across PAIR’s portfolio, the approach is consistent: transform complex, often opaque machine learning concepts into approachable, interactive experiences. “Measuring Fairness” continues this tradition by making fairness—a typically abstract and controversial subject—tangible and manipulable, without oversimplifying its nuances.</p>
    </section>

    <section>
      <h2>Personal Assessment</h2>
      <p>As a pedagogical tool, “Measuring Fairness” is remarkably effective. It balances conceptual rigor with visual and interactive clarity, making it accessible to a wide range of users. Its modular design, both narratively and technically, allows each user to construct their own understanding of fairness and its implications. The interface is elegant, intuitive, and purposefully non-prescriptive.</p>
      <p>However, the project is not without its limitations. Its use of simulated data removes the discussion from real-world sociopolitical dynamics. Without labels such as race, gender, or class, it avoids controversial specifics, which may make the trade-offs seem sterile or disembodied. This can obscure the urgency and gravity of fairness debates in real applications like credit scoring, policing, or healthcare.</p>
      <p>Additionally, by not taking a normative stance, the project risks appearing neutral in contexts where neutrality itself can reinforce existing power structures. Given that the project is produced by Google—a company that operates many real-world algorithmic systems—this lack of positioning can be seen as both strategic and ethically ambiguous.</p>
      <p>Despite these critiques, the project is a landmark in interactive data pedagogy. It succeeds in making abstract, ethically charged concepts approachable and fosters a deeper, more reflective engagement with algorithmic decision-making.</p>
    </section>
  </div>
</main>
</body>
</html>

    </div>
</main>
    <footer>
        &copy; 2025 Fairness Explained. All rights reserved.
    </footer>
</body>
</html>
